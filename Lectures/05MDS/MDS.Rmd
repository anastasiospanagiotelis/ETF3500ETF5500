---
title: "Multidimensional Scaling"
subtitle: "High Dimensional Data Analysis"
author: "Anastasios Panagiotelis"
date: "Lecture 5"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"mtheme.css","mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

class: inverse, center, middle

# Motivation

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
require(MASS)
require(magrittr)
require(tidyverse)
require(plotly)
require(widgetframe)
require(animation)
require(scatterplot3d)
require(DT)
require(rgl)
require(knitr)
require(kableExtra)
require(ggmap)
require(htmlwidgets)
rm(list=ls())
```

---

# Motivation

- Previously we looked at the concept of *distance* between observations.<!--D-->
--

- We looked at our usual understanding of distance known as *Euclidean* distance.<!--D-->
--

- We also looked at higher dimensional versions of Euclidean distance.<!--D-->
--

- Other distance metrics including *Jaccard* distance can be used for categorical data.

---

# Can we see distance?

- Suppose we have $n$ observations and the distance between each possible pair of observations.<!--D-->
--

- A scatterplot shows whether observations are close together or far apart.<!--D-->
--

- This works nicely when there are 2 variables.<!--D-->

---

# Higher-dimensional plots

- Suppose we have $p$ variables where $p$ is large.
--

- Consider $p$-dimensional Euclidean distances.<!--D-->
--

- Can we represent these using just $2$-dimensions?<!--D-->
--

- Unfortunately the answer is no...<!--D-->
--

- ... but we can get a good approximation

---

# Multidimensional Scaling

- Multidimensional scaling (MDS) finds a low (usually 2) dimensional representation.<!--D-->
--

- The pairwise 2D Euclidean distances in this representation should be as *close* as possible to the original distances.<!--D-->
--

- The meaning of *close* can vary since there are different ways to do MDS.<!--D-->
--

- However MDS always begins with a matrix of distances and ends with a low dimensional representation that can be plotted. 

---

# An optical illusion with Beyonce

![Beyonce and the Eiffel Tower](beyonce.jpg)


---

# Why does the illusion work?

- The photo is a 2D representation of a 3D reality.<!--D-->
--

- In reality the distance between Beyonce's hand and the Eiffel Tower is large.<!--D-->
--

- In the 2D photo, this distance is small.<!--D-->
--

- This is a misleading representation to understand the distance between Beyonce's hand and the Eiffel Tower.<!--D-->
--

- A much more informative representation could be found by *rotation*.

---

# Why do we care?

- An important issue in business is to profile the market.  For
example<!--D-->
--

  + Which products do customers perceive to be similar to one
another?<!--D-->
--

  + Who is my closest competitor?<!--D-->
--

  + Are there ‘gaps’ in the market, where a new product can be introduced?<!--D-->
--

- Multidimensional Scaling can help us to produce a simple visualisation that can address these questions.

---

# Beer Example

```{r beer, echo=FALSE,fig.width=8,fig.height=7}
Beer<-readRDS('Beer.rds') 

#Unstandardised Numeric Data

filter(Beer,rating=='Fair')%>% #Only high price
  select_if(is.numeric)%>%
  scale%>%
  dist->d

filter(Beer,rating=='Fair')%>% #Only high price
  pull(beer)%>%
  abbreviate(6)->
  attributes(d)$Labels

mdsout<-cmdscale(d)
colnames(mdsout)<-c('PC1','PC2')

mdsout%>%
  as.data.frame()%>%
  rownames_to_column('BeerName')%>%
  ggplot(aes(x=PC1,y=PC2,label=BeerName))+geom_text(size=10)+coord_cartesian(xlim=c(-3.5,3.5))


```

---

# Beer Example

- The plot on the previous slide is an MDS
solution for the beer dataset.<!--D-->
--

- The data are 5-dimensional so we cannot use a scatter plot.<!--D-->
--

- MDS shows Olympia Gold Light and Pabst Extra Light are similar (both
light beers).<!--D-->
--

- This also suggest that there is a low number of competitors with St Pauli Girl.<!--D-->
--

- This may also reflect that the attributes of St Pauli Girl are not desired by customers.<!--D-->
--

- How did we get the plot?

---

# Beer Data

```{r summary,echo=FALSE}
Beer%>%filter(rating=='Fair')%>%kable%>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"))%>%
  scroll_box(height="500px")
```

---

# Details

- To keep the example simple only the beers rated *fair* are used<!--D-->
--

- In general, all the beers can be used.<!--D-->
--

- Also to keep things simple we only consider the metric variables so that we can use Euclidean distance.<!--D-->
--

- In general, we can use distance metrics that work for categorical data.

---

# Metric Variables

- After standardising, Euclidean distances are formed between every possible pair of beers.<!--D-->
--

- For example, the distance between Blatz and Tuborg is given by<!--D-->
--


$$\delta\left(\mbox{Blatz},\mbox{Tbrg}\right)=\sqrt{\sum\limits_{h=1}^5(\mbox{Blatz}_h-\mbox{Tbrg}_h)^2}$$
Both the notation $\delta_{ij}$ and $\delta(i,j)$ will  be used interchangeably

---

# Doing it in R

To obtain the distance matrix in R
```{r, preprocess, echo=TRUE}
filter(Beer,rating=='Fair')%>% #Only fair beers
  select_if(is.numeric)%>% #Only metric data
  scale%>% #Standardise
  dist->delta #Distance

filter(Beer,rating=='Fair')%>% #Only fair beers
  pull(beer)%>% #Get beer names
  abbreviate(6)-> #Abbreviate
  attributes(delta)$Labels #Assign to d
```

---

#MDS in R

We can do what is known as **classical** MDS in R using the `cmdscale` function

```{r, cmds, echo =TRUE}
mdsout<-cmdscale(delta)
```

```{r, cmdsprint, echo =FALSE}
mdsout%>%kable()%>%kable_styling(bootstrap_options = c("striped","hover","condensed"))%>%
  scroll_box(height="200px")
```

---

# Two new variables

- We have just created two new variables for visualising the distances.<!--D-->
--

- The distances that we visualise will be 2-dimensional distances.  For example<!--D-->
--


$$\begin{align}d&(\mbox{Blatz},\mbox{Tbrg})=\\&\sqrt{(-0.339-0.808)^2+(1.493-0.703)^2}\end{align}$$

---

# Not exact

- In this example $d(\mbox{Blatz},\mbox{Tuborg})=1.3927$ while $\delta(\mbox{Blatz},\mbox{Tuborg})=1.4762$.  Notice that<!--D-->
--


$$
d(\mbox{Blatz},\mbox{Tuborg})\neq \delta(\mbox{Blatz},\mbox{Tuborg})
$$

- But they are close.

---

# Getting the plot

```{r getplot, fig.height=4,echo=TRUE,warning=FALSE}

mdsout%>%
  as_tibble%>% 
  ggplot(aes(x=V1,y=V2))+geom_point()
```

---

# Getting the plot with names

```{r getplotn, fig.height=4, echo=TRUE,warning=FALSE}
mdsout%>%
  as_tibble(rownames='BeerName')%>%
  ggplot(aes(x=V1,y=V2,label=BeerName))+geom_text()
```

---

# The math behind classical MDS

- In *classical* MDS the objective is to minimise strain<!--D-->
--


$$\mbox{Strain}=\sum\limits_{i=1}^{n-1}\sum\limits_{j>i}(\delta^2_{ij}-d^2_{ij})$$

- Note that the $\delta_{ij}$ are high dimensional distances that come from the true data.
--

- The $d_{ij}$ are low dimensional distances that come from the solution.

---

# When can this be solved?

- The above problem has a tractable solution when Euclidean distance is used.
--

- This solution depends on an Eigenvalue decomposition.
--

- This solution *rotates* the points until we get a 2D view that represents the true distances as accurately as possible.

---

# Summary

- When Euclidean distance is used the solution provided by classical MDS:
--

  - Minimises the strain.
--

  - Results in eigenvalues that are all positive
--

- Can we use classical MDS when distances are non-Euclidean?

---

# An example: Road distances

- Suppose that we have the road distances between different cities in Australia.<!--D-->
--

- The road distances are non-Euclidean since roads can be quite wiggly.<!--D-->
--

- We want to create a 2-dimensional map with the locations of the cities using only these road distances.  
--

- Classical MDS can give an approximation that is quite close to a real map.

---

# Road Distances


```{r setd,echo=FALSE}
dm<-matrix(c(
0,0,0,0,0,0,0,0,  
1717,0,0,0,0,0,0,0,
2546,996,0,0,0,0,0,0,
3054,1674,868,0,0,0,0,0,
3143,2063,1420,728,0,0,0,0,
5954,4348,4144,3452,2724,0,0,0,
2727,3415,4000,3781,3053,4045,0,0,
2324,3012,2644,2270,1542,3630,1511,0
),
8,8,byrow = TRUE)
doz<-as.dist(dm)
attributes(doz)$Labels<-c("Cairns",
                         "Brisbane",
                         "Sydney",
                         "Melbourne",
                         "Adelaide",
                         "Perth",
                         "Darwin",
                         "Alice Springs")                  
kable(as.matrix(doz))%>% kable_styling(bootstrap_options = c("striped","hover","condensed"))%>%
  scroll_box(height="500px")

```

---

# Australia


```{r ozmap,eval=TRUE,cache=TRUE, echo=FALSE,message=FALSE, warning=FALSE}

oz<-get_map(location=c(134,-23.5),zoom=4,maptype = 'watercolor')
coords<-data_frame(city=c("Sydney",
                  "Melbourne",
                  "Brisbane",
                  "Perth",
                  "Adelaide",
                  "Cairns",
                  "Alice Springs",
                  "Darwin"),
           lng=c(151.2093,
                 144.9631,
                 153.025124,
                 115.860457,
                 138.600746,
                 145.7781,
                 133.8807,
                 130.845642),
           lat=c(-33.8688,
                 -37.8136,
                 -27.469771,
                 -31.950527,
                 -34.928499,
                 -16.9186,
                 -23.6980,
                 -12.463440)
           )
g<-ggmap(oz)+geom_text(data=coords,aes(x=lng, y=lat,label=city),size=8,nudge_y = 1.5)+geom_point(data=coords,aes(x=lng, y=lat),size=6)
g
```

---

# MDS Solution

```{r,mdsoz,echo=FALSE}
cmdscale(doz)->dozout
colnames(dozout)<-c('lng','lat')
dozout%>%
  as.data.frame()%>%
  rownames_to_column("city")->
  ozmdsout

ggplot(ozmdsout,aes(x=lng,y=lat,label=city))+geom_text(size=8)+coord_cartesian(xlim=c(-3500,3000))

```

---

# Rotate

```{r,mdsoz2,echo=FALSE,eval=TRUE}

cmdscale(doz)->dozout
th<-0.65
dozout<-dozout/130
rot<-matrix(c(cos(th),-sin(th),sin(th),cos(th)),2,2)
dozout<-dozout%*%rot
cent<-apply(coords[,2:3],2,mean)+c(1,-1)
for (i in 1:8){dozout[i,]<-dozout[i,]+cent}


colnames(dozout)<-c('lng','lat')
dozout%>%
  as.data.frame()%>%
  rownames_to_column("city")->
  ozmdsout

ggplot(ozmdsout,aes(x=lng,y=lat,label=city))+geom_text(size=8)+coord_cartesian(xlim=c(115,156))
```
---

# Back with Map

```{r, mdssolmap,echo=FALSE,eval=TRUE}
ggmap(oz)+geom_text(data=ozmdsout,aes(x=lng, y=lat,label=city),size=8,nudge_y = 1.5)+geom_point(data=ozmdsout,aes(x=lng, y=lat),size=6)
```

---

#Rotating

- Once a solution is available, we rotate the points within 2 dimensions.<!--D-->
--

  - The 2D rotation does not change any of the distances.<!--D-->
--

  - It can help us to interpret the axes.<!--D-->
--

- In the previous example the x-axis represents East-West direction and the y-axis represents North-South.

---
class: inverse, center, middle

# Evaluating MDS

---

# How good is this representation?

- In theory, as long as the original distances are Euclidean, strain is minimised.
--

- What if the optimal solution is still bad?
--

- Use two goodness of fit measures.<!--D-->
--

- Think of these in a similar fashion to R square in regression modelling.

---

# Goodness of Fit Measures?

- These values depend on the eigenvalues
$$\mbox{GF}_1=\frac{\sum\limits_{i=1}^2 |\lambda_i|}{\sum\limits^n_{i=1}|\lambda_i|}\,
\mbox{GF}_2=\frac{\sum\limits_{i=1}^2 max(0,\lambda_i)}{\sum\limits^n_{i=1}max(0,\lambda_i)}$$
- For Euclidean distances $\delta_{ij}$ eigenvalues are always positive and $\mbox{GF}_1=\mbox{GF}_2$.

---

# Beer Example 

- In R obtain GoF using the option `eig=TRUE` in the `cmdscale` function<!--D-->
--

- For the Beer data.
```{r,mdseig,echo=TRUE}
mdsout<-cmdscale(delta,eig=TRUE)
str(mdsout$GOF)
```

---

# GoF Measure

- You may notice that the GoF measures are the same.
--

- This is always the case when Euclidean distance is used.
--

- This arises since all eigenvalues are positive when the distance matrix is based on Euclidean distance.

---

# Non-Euclidean distances

- In theory non-Euclidean distances can lead to negative eigenvalues.  In this case:<!--D-->
--

  + Classical MDS may not minimise Strain.<!--D-->
--

  + It minimises a slightly different function of the distances.
--

  + Two fit measures will differ.<!--D-->
--

- Overall, we can use classical MDS for non-Euclidean distance but must be more careful.

---

# Australia data


```{r, roadeig,echo=TRUE}
cmdscale(doz,eig=TRUE)->dozout
str(dozout$eig)
str(dozout$eig[6:8])
str(dozout$GOF)
```

---

# Evaluating the Result

- There are negative eigenvalues.<!--D-->
--

  + This occurs since road distances are not Euclidean<!--D-->
--

  + This also implies that classical MDS does not minimise strain.<!--D-->
--

- Both goodness of fit measures are quite high.<!--D-->
--

  + The solution is an accurate representation.

---

# Another example: Cheese

The following example comes from ‘Multidimensional Scaling of Sorting Data Applied to Cheese Perception’, Food Quality and Preference,6, pp.91-98.  The purpose of this study was to visualise the difference between types of cheese.


---

# Another example: Cheese

- The motivation is to investigate the similarities and differences between types of cheese.<!--D-->
--

- In principle one could measure attributes of the cheese.<!--D-->
--

- However the purpose of this study was to ask customers about their perceptions.<!--D-->
--

- How do we ask customers about distances?<!--D-->
--

- Could you walk out on to the street and ask someone about the Euclidean distance between Brie and Camembert?

---

# Constructing the Survey

- Customers can be asked:<!--D-->
--

- On a scale of 1 to 10 with 1 being the most similar and 10
being the most different, how similar are the following
cheeses<!--D-->
--

  + Brie and Camembert<!--D-->
--

  + Brie and Roquefort<!--D-->
--

  + Camembert and Roquefort
--

- The dissimilarity scores can be averaged over all
customers and used in an MDS<!--D-->
--

- This is not a good method when there is a large number of
products.

---

# A more feasible approach

- In the study there are 16 cheeses therefore 120 possible
pairwise comparisons.<!--D-->
--

- It is not practical to ask survey participants to make 120 comparisons!<!--D-->
--

- Instead of being asked to make so many comparisons, customers were asked to put similar
cheeses into groups.<!--D-->
--

- Proportion of customers with two cheeses in same group is a similarity score.<!--D-->
--

- Proportion of customers with two cheeses in different groups is a dissimilarity score.

---

# Consider four customers

- Suppose there are four customers sorting cheeses<!--D-->
--

  + Customer A: Brie and Camembert together, Roquefort and
Blue Vein together<!--D-->
--

  + Customer B: Roquefort and Blue Vein together, all others
separate<!--D-->
--

  + Customer C: All cheeses in their own category<!--D-->
--

  + Customer D: All cheeses in one category
    
---

# Comparisons

- Customer A and D have Brie and Camembert in the same group, customers B and C have them in different groups.<!--D-->
--

  + The distance between Brie and Camembert is 0.5.<!--D-->
--

- Customer A, B and D have Roquefort and Blue Vein in the same group, customer C has them in different groups.<!--D-->
--

  + The distance between Roquefort and Blue Vein is 0.25.

---

# MDS

- The study on cheese did not use classical MDS but something called *Kruskals algorithm*.
- There are many alternatives to classical MDS.
- We now briefly cover some of the ideas behind them.

---
class: inverse, center, middle

# Beyond Classical MDS

---

# Beyond Classical MDS

- Classical MDS is designed to minimise Strain.<!--D-->
--

- An alternative  objective function called Stress can be minimised instead<!--D-->
--


$$\mbox{Stress}=\sum\limits_{i=1}^{n-1}\sum\limits_{j>i}\frac{(\delta_{ij}-d_{ij})^2}{\delta_{ij}}$$
--

- The difference between $\delta_{ij}$ and $d_{ij}$ acts like an error.
--

- The $\delta_{ij}$ on the denominator acts as a weight

---

#Weighting

- For large $\delta$ observations are far away in the original space.
--

  - For these pairs errors are more easily tolerated.
--

- For small $\delta$ observations are close in the original space.
--

  - For these pairs errors are not tolerated.
--

- The most accuracy is achieved for nearby points
--

- The local structure is preserved.

---

# Sammon mapping

- The Sammon mapping is solved by numerical optimisation.
--

- It is different from the classical solution
--

  - It is not based on an eigenvalue decomposition
--

  - It is not based on rotation
--

  - It is a non-linear mapping.

---

# Example

- Consider the case where points are in 2D space and the aim is to summarise them in 1D space (across a line).
- The specific problem of doing multidimensional scaling where the lower dimension is 1 is called *seriation*.
- It provides a ranking of the observations.
--

- In marketing it can be used to elicit preferences.

---

# Original Data

```{r,echo=FALSE,message=FALSE,warning=FALSE,error=FALSE}
set.seed(1)
xg<-seq(0,1,0.005)

signal<-1.2*sin((2*pi*xg)/2)

yg<-signal+0.01*rnorm(length(xg))



raw<-cbind(xg,yg)
theta<-pi/4
rotm<-matrix(c(cos(theta),-sin(theta),sin(theta),cos(theta)),2,2)
rot<-raw%*%rotm
dd<-dist(rot)
pca<-prcomp(rot)
cmds<-cmdscale(dd,k = 1)
smds<-sammon(dd,k=1,trace = FALSE)
smds<-sammon(dd,y = matrix(-xg,ncol = 1),k=1,trace = FALSE)
df<-tibble(rank=xg,
           x=-rot[,1],
           y=rot[,2],
           classic=cmds,
           sammon=smds$points,
           xrot=pca$x[,1],
           yrot=pca$x[,2])
ggplot(df,aes(x=x,y=y))+geom_point()+coord_fixed()
```

---

# Original Data

```{r,echo=FALSE}
ggplot(df,aes(x=x,y=y,col=rank))+geom_point()+scale_color_viridis_c()+coord_fixed()
```

---

# Rotate (Classical Solution)

```{r,echo=FALSE}
ggplot(df,aes(x=xrot,y=yrot,col=rank))+geom_point()+scale_color_viridis_c()+coord_fixed()
```

---

# Keep 1 Dimension

```{r,echo=FALSE}
ggplot(df,aes(x=classic,y=0,col=rank))+geom_point()+scale_color_viridis_c()
```


---

# Rug plot (classical solution)

```{r,echo=FALSE}
ggplot(df,aes(x=classic,col=rank))+geom_rug()+scale_color_viridis_c()
```


---

# Sammon Mapping

```{r,echo=FALSE}
ggplot(df,aes(x=sammon,col=rank))+geom_rug()+scale_color_viridis_c()

```

---

# Discussion

- Classical MDS cannot account for non-linearity.  
- The dark blue and yellow points are represented as close to one another.
- Sammon does account for non-linearity.
- The blue and yellow points are represented as far apart.
- Although they are not so far apart in the original space, these observations are downweighted relative to the local structure.


---

# Kruskal algorithm

- Kruskal's algorithm minimises a slightly different criterion.
--

- This is still often called *stress*, which is admittedly confusing.
--

- Kruskal's algorithm is implemented in R using the `isoMDS` function from the `MASS` package.

---

# Monotone tranformations

- Kruskal's algorithm is invariant to monotone transformations of the distances.
--

- By *monotone transformation* we mean any function of the distance that is either constantly increasing or decreasing.
--

  - Exponential function is monotone
--

  - Sine function is not monotone
--

- By *invariant* we mean that the solution provided by Kruskal's does not change if we transform the input distances.

---

# Example

```{r, echo=TRUE,message=FALSE}
library(MASS)
isoMDS(d)->kBeer

```

---

# Make plot

```{r, echo=TRUE,eval=FALSE,message=FALSE}
kBeer$points%>%
  as_tibble()%>% 
  ggplot(aes(x=V1,y=V2))+
  geom_point(size=10)
```

---

# Make plot

```{r, echo=FALSE,,eval=TRUE,message=FALSE}
kBeer$points%>%
  as_tibble()%>%
  ggplot(aes(x=V1,y=V2))+
  geom_point(size=10)

```

---

# Squared distances

```{r, echo=TRUE,message=FALSE}
isoMDS(d^2)->kBeer2

```

---

# Solution

```{r, echo=FALSE,,eval=TRUE,message=FALSE}
kBeer2$points%>%
  as_tibble()%>%
  ggplot(aes(x=V1,y=V2))+
  geom_point(size=10)

```

---

# Comparison

- Squaring the distances provides the same solution with two caveats:
--

  - The stress is slightly different.  Numerical optimisation can vary a little depending on starting values. 
--

  - The points in one plot are slightly rotated compared to the other.
--

- Why is the invariance to monotone tranformations important?

---

# Non metric MDS

- In some cases, the distance themselves are not metric but ordinal.<!--D-->
--

- Suppose we only know<!--D-->
--

$$\delta_{\mbox{Bri.},\mbox{Cam.}}<
\delta_{\mbox{Roq.},\mbox{Cam.}}<
\delta_{\mbox{Roq.},\mbox{Bri.}}$$
--

- Brie and Roquefort are *more different* compared to Brie and Camembert.<!--D-->
--

- We do not know *how big* the distance between Brie and Roquefort is compared to the distance between Brie and Camembert. 

---

# Non-metric MDS

- In this case we minimise Stress subject to constraints, e.g.<!--D-->
--

$$\hat{\delta}_{\mbox{Bri.},\mbox{Cam.}}<
\hat{\delta}_{\mbox{Roq.},\mbox{Cam.}}<
\hat{\delta}_{\mbox{Roq.},\mbox{Bri.}}$$

---

# Non-metric MDS

- Taking the ranks is an example of a monotone transformation.
--

- Therefore the solution of isoMDS only requires the ranks of the distances and not the distances themselves.
--

- This is a very useful algorithm for marketing since survey participants cannot easily and reliable assign numbers to the difference between products.<!--D-->

---

# Modern MDS

- Methods for finding a low dimensional representation of high-dimensional data continue to be used today
--

- These mostly go by the name of **manifold learning** methods
--

- They are not only used for visualisation
--

- The low-dimensional co-ordinates can also be used as features in classification and regression.

---

# Examples

- Local Linear Embedding (LLE)
- IsoMap
- Laplacian Eigenmap
- t SNE
- Kohonen Map
- ...
--

- ... and others.

---

# Properties

- For most of the modern methods two characteristics are common.
--

  - The idea that local structure should be preserved.  The first step of many algorithms is to find nearest neighbours of each point.
--

  - In many algorithms an eigenvalue decomposition forms part of the solution as is the case in classic MDS.